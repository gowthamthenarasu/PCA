{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Step 1: Install & Import Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gowtham.T\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gowtham.T\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# Install necessary libraries (if not already installed)\n",
    "#!pip install nltk spacy gensim\n",
    "\n",
    "# Download necessary NLTK data\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Load Spacy model\n",
    "import spacy\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "# Import required libraries\n",
    "import string\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from collections import Counter\n",
    "from gensim.models import Word2Vec\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !python -m spacy download en_core_web_sm\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ### Step 2: Read & Display the Text File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Text:\n",
      "\n",
      "Artificial Intelligence (AI) has evolved significantly over the past few decades. \n",
      "Early AI systems were rule-based, meaning they followed predefined instructions and couldn’t adapt to new information. \n",
      "However, with the rise of Machine Learning, AI began to learn from data instead of relying on fixed rules.\n",
      "\n",
      "One major breakthrough was the development of Neural Networks, which mimic the human brain. \n",
      "These networks improved AI’s ability to recognize patterns, understand language, and generate hu\n"
     ]
    }
   ],
   "source": [
    "# Read the text file\n",
    "with open(\"sample.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    text = file.read()\n",
    "\n",
    "# Display the original text\n",
    "print(\"Original Text:\\n\")\n",
    "print(text[:500])  # Print first 500 characters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\Gowtham.T\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Gowtham.T\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Preprocess the Text (Lowercasing, Removing Punctuation, Tokenization, Stopwords, Lemmatization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nltk.data.path.append(\"C:/Users/Gowtham.T/AppData/Roaming/nltk_data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Gowtham.T\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Filtered Tokens (First 20): ['example', 'sentence', 'quick', 'brown', 'fox', 'jumped', 'lazy', 'dog']\n",
      "\n",
      "Lemmatized Tokens (First 20): ['example', 'sentence', 'quick', 'brown', 'fox', 'jump', 'lazy', 'dog']\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import spacy\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('stopwords')  # For stopwords\n",
    "\n",
    "# Load SpaCy model\n",
    "nlp = spacy.load(\"en_core_web_sm\")  # Load SpaCy's small English model\n",
    "\n",
    "# Sample text\n",
    "text = \"This is an example sentence! The quick brown fox jumped over the lazy dog.\"\n",
    "\n",
    "# Convert text to lowercase\n",
    "text = text.lower()\n",
    "\n",
    "# Remove punctuation\n",
    "text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "# Tokenization using SpaCy (instead of NLTK word_tokenize)\n",
    "doc = nlp(text)\n",
    "tokens = [token.text for token in doc]\n",
    "\n",
    "# Remove stopwords\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "filtered_tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "# Lemmatization using SpaCy\n",
    "lemmatized_tokens = [token.lemma_ for token in nlp(\" \".join(filtered_tokens))]\n",
    "\n",
    "# Display results\n",
    "print(\"\\nFiltered Tokens (First 20):\", filtered_tokens[:20])\n",
    "print(\"\\nLemmatized Tokens (First 20):\", lemmatized_tokens[:20])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use the Word2Vec model from Gensim."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vector for 'dog': \n",
      " [-5.3622725e-04  2.3643136e-04  5.1033497e-03  9.0092728e-03\n",
      " -9.3029495e-03 -7.1168090e-03  6.4588725e-03  8.9729885e-03\n",
      " -5.0154282e-03 -3.7633716e-03  7.3805046e-03 -1.5334714e-03\n",
      " -4.5366134e-03  6.5540518e-03 -4.8601604e-03 -1.8160177e-03\n",
      "  2.8765798e-03  9.9187379e-04 -8.2852151e-03 -9.4488179e-03\n",
      "  7.3117660e-03  5.0702621e-03  6.7576934e-03  7.6286553e-04\n",
      "  6.3508903e-03 -3.4053659e-03 -9.4640139e-04  5.7685734e-03\n",
      " -7.5216377e-03 -3.9361035e-03 -7.5115822e-03 -9.3004224e-04\n",
      "  9.5381187e-03 -7.3191668e-03 -2.3337686e-03 -1.9377411e-03\n",
      "  8.0774371e-03 -5.9308959e-03  4.5162440e-05 -4.7537340e-03\n",
      " -9.6035507e-03  5.0072931e-03 -8.7595852e-03 -4.3918253e-03\n",
      " -3.5099984e-05 -2.9618145e-04 -7.6612402e-03  9.6147433e-03\n",
      "  4.9820580e-03  9.2331432e-03 -8.1579173e-03  4.4957981e-03\n",
      " -4.1370760e-03  8.2453608e-04  8.4986202e-03 -4.4621765e-03\n",
      "  4.5175003e-03 -6.7869602e-03 -3.5484887e-03  9.3985079e-03\n",
      " -1.5776526e-03  3.2137157e-04 -4.1406299e-03 -7.6826881e-03\n",
      " -1.5080082e-03  2.4697948e-03 -8.8802696e-04  5.5336617e-03\n",
      " -2.7429771e-03  2.2600652e-03  5.4557943e-03  8.3459532e-03\n",
      " -1.4537406e-03 -9.2081428e-03  4.3705525e-03  5.7178497e-04\n",
      "  7.4419081e-03 -8.1328274e-04 -2.6384138e-03 -8.7530091e-03\n",
      " -8.5655687e-04  2.8265631e-03  5.4014288e-03  7.0526563e-03\n",
      " -5.7031214e-03  1.8588197e-03  6.0888636e-03 -4.7980510e-03\n",
      " -3.1072604e-03  6.7976294e-03  1.6314756e-03  1.8991709e-04\n",
      "  3.4736372e-03  2.1777749e-04  9.6188262e-03  5.0606038e-03\n",
      " -8.9173904e-03 -7.0415605e-03  9.0145587e-04  6.3925339e-03]\n",
      "\n",
      "Most similar words to 'dog': [('example', 0.09291718155145645), ('sentence', 0.01613466814160347), ('lazy', -0.010839181020855904), ('brown', -0.027750367298722267), ('jumped', -0.052346743643283844)]\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "# Example: Word2Vec model training\n",
    "# First, we need to prepare our data in the form of sentences (a list of tokens)\n",
    "sentences = [filtered_tokens]  # You can extend this to more sentences for training.\n",
    "\n",
    "# Train the Word2Vec model\n",
    "model = Word2Vec(sentences, vector_size=100, window=5, min_count=1, sg=0)\n",
    "\n",
    "# Get vector for a word (for example: 'dog')\n",
    "word_vector = model.wv['dog']\n",
    "print(f\"Vector for 'dog': \\n\", word_vector)\n",
    "\n",
    "# Find similar words to a given word (for example: 'dog')\n",
    "similar_words = model.wv.most_similar('dog', topn=5)\n",
    "print(\"\\nMost similar words to 'dog':\", similar_words)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Applying Bag of Words (BoW)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulary: ['brown' 'dog' 'example' 'fox' 'jumped' 'lazy' 'quick' 'sentence']\n",
      "\n",
      "Bag of Words matrix:\n",
      " [[1 1 1 1 1 1 1 1]]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Sample documents (You can replace it with your tokenized sentences)\n",
    "documents = [\" \".join(filtered_tokens)]  # Use multiple sentences here as a list\n",
    "\n",
    "# Create the CountVectorizer\n",
    "vectorizer = CountVectorizer()\n",
    "\n",
    "# Fit and transform the data (Converts text to BoW representation)\n",
    "X = vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (vocabulary) from the model\n",
    "vocabulary = vectorizer.get_feature_names_out()\n",
    "print(\"\\nVocabulary:\", vocabulary)\n",
    "\n",
    "# Display the BoW matrix\n",
    "print(\"\\nBag of Words matrix:\\n\", X.toarray())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  TF-IDF (Term Frequency-Inverse Document Frequency)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "TF-IDF Matrix:\n",
      " [[0.35355339 0.35355339 0.35355339 0.35355339 0.35355339 0.35355339\n",
      "  0.35355339 0.35355339]]\n",
      "\n",
      "TF-IDF Vocabulary: ['brown' 'dog' 'example' 'fox' 'jumped' 'lazy' 'quick' 'sentence']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Sample documents (You can use more documents here)\n",
    "documents = [\" \".join(filtered_tokens)]  # Add more documents for better results\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "tfidf_vectorizer = TfidfVectorizer()\n",
    "\n",
    "# Fit and transform the documents\n",
    "X_tfidf = tfidf_vectorizer.fit_transform(documents)\n",
    "\n",
    "# Get the feature names (vocabulary) from the model\n",
    "vocabulary_tfidf = tfidf_vectorizer.get_feature_names_out()\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"\\nTF-IDF Matrix:\\n\", X_tfidf.toarray())\n",
    "\n",
    "# Display vocabulary (words with corresponding TF-IDF scores)\n",
    "print(\"\\nTF-IDF Vocabulary:\", vocabulary_tfidf)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
